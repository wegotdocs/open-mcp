import { z } from "zod"

export const inputParamsSchema = {
  "version_id": z.string().describe("ID of the Evaluator version to log against.").optional(),
  "environment": z.string().describe("Name of the Environment identifying a deployed version to log to.").optional(),
  "path": z.string().describe("Path of the Evaluator, including the name. This locates the Evaluator in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.").optional(),
  "id": z.string().describe("ID for an existing Evaluator.").optional(),
  "start_time": z.string().datetime({ offset: true }).describe("When the logged event started.").optional(),
  "end_time": z.string().datetime({ offset: true }).describe("When the logged event ended.").optional(),
  "output": z.string().describe("Generated output from the LLM. Only populated for LLM Evaluator Logs.").optional(),
  "created_at": z.string().datetime({ offset: true }).describe("User defined timestamp for when the log was created. ").optional(),
  "error": z.string().describe("Error message if the log is an error.").optional(),
  "provider_latency": z.number().describe("Duration of the logged event in seconds.").optional(),
  "stdout": z.string().describe("Captured log and debug statements.").optional(),
  "provider_request": z.record(z.any()).describe("<llm-instruction>This part of the input schema is truncated. If you want to pass the property `provider_request` to the tool, first call the tool `expandSchema` with \"/properties/provider_request\" in the list of pointers. This will return the expanded input schema which you can then use in the tool call. You may have to call `expandSchema` multiple times if the schema is nested.</llm-instruction>\n<property-description>Raw request sent to provider. Only populated for LLM Evaluator Logs.</property-description>").optional(),
  "provider_response": z.record(z.any()).describe("<llm-instruction>This part of the input schema is truncated. If you want to pass the property `provider_response` to the tool, first call the tool `expandSchema` with \"/properties/provider_response\" in the list of pointers. This will return the expanded input schema which you can then use in the tool call. You may have to call `expandSchema` multiple times if the schema is nested.</llm-instruction>\n<property-description>Raw response received the provider. Only populated for LLM Evaluator Logs.</property-description>").optional(),
  "inputs": z.record(z.any()).describe("<llm-instruction>This part of the input schema is truncated. If you want to pass the property `inputs` to the tool, first call the tool `expandSchema` with \"/properties/inputs\" in the list of pointers. This will return the expanded input schema which you can then use in the tool call. You may have to call `expandSchema` multiple times if the schema is nested.</llm-instruction>\n<property-description>The inputs passed to the prompt template.</property-description>").optional(),
  "source": z.string().describe("Identifies where the model was called from.").optional(),
  "metadata": z.record(z.any()).describe("<llm-instruction>This part of the input schema is truncated. If you want to pass the property `metadata` to the tool, first call the tool `expandSchema` with \"/properties/metadata\" in the list of pointers. This will return the expanded input schema which you can then use in the tool call. You may have to call `expandSchema` multiple times if the schema is nested.</llm-instruction>\n<property-description>Any additional metadata to record.</property-description>").optional(),
  "log_status": z.string().optional(),
  "parent_id": z.string().describe("Identifier of the evaluated Log. The newly created Log will have this one set as parent."),
  "source_datapoint_id": z.string().describe("Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.").optional(),
  "trace_parent_id": z.string().describe("The ID of the parent Log to nest this Log under in a Trace.").optional(),
  "user": z.string().describe("End-user ID related to the Log.").optional(),
  "b_environment": z.string().describe("The name of the Environment the Log is associated to.").optional(),
  "save": z.boolean().describe("Whether the request/response payloads will be stored on Humanloop.").optional(),
  "log_id": z.string().describe("This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.").optional(),
  "output_message": z.string().optional(),
  "judgment": z.union([z.boolean(), z.string(), z.array(z.string()), z.number()]).describe("Evaluator assessment of the Log.").optional(),
  "marked_completed": z.boolean().describe("Whether the Log has been manually marked as completed by a user.").optional(),
  "spec": z.union([z.object({ "arguments_type": z.string(), "return_type": z.string(), "attributes": z.record(z.any()).describe("Additional fields to describe the Evaluator. Helpful to separate Evaluator versions from each other with details on how they were created or used.").optional(), "options": z.array(z.object({ "name": z.string().describe("The name of the option."), "valence": z.string().optional() })).describe("The options that can be applied as judgments. Only for Evaluators with `return_type` of 'boolean', 'select' or 'multi_select'.").optional(), "number_limits": z.string().optional(), "number_valence": z.string().optional(), "evaluator_type": z.literal("llm").describe("The type of the evaluator."), "prompt": z.string().optional() }), z.object({ "arguments_type": z.string(), "return_type": z.string(), "attributes": z.record(z.any()).describe("Additional fields to describe the Evaluator. Helpful to separate Evaluator versions from each other with details on how they were created or used.").optional(), "options": z.array(z.object({ "name": z.string().describe("The name of the option."), "valence": z.string().optional() })).describe("The options that can be applied as judgments. Only for Evaluators with `return_type` of 'boolean', 'select' or 'multi_select'.").optional(), "number_limits": z.string().optional(), "number_valence": z.string().optional(), "evaluator_type": z.literal("python").describe("The type of the evaluator."), "code": z.string().describe("The code for the Evaluator. This code will be executed in a sandboxed environment.") }), z.object({ "arguments_type": z.string(), "return_type": z.enum(["select","multi_select","text","number","boolean"]).describe("The type of the return value of the Evaluator."), "attributes": z.record(z.any()).describe("Additional fields to describe the Evaluator. Helpful to separate Evaluator versions from each other with details on how they were created or used.").optional(), "options": z.array(z.object({ "name": z.string().describe("The name of the option."), "valence": z.string().optional() })).describe("The options that can be applied as judgments.").optional(), "number_limits": z.string().optional(), "number_valence": z.string().optional(), "evaluator_type": z.literal("human").describe("The type of the evaluator."), "instructions": z.string().describe("Instructions and guidelines for applying judgments.").optional() }), z.object({ "arguments_type": z.string(), "return_type": z.string(), "attributes": z.record(z.any()).describe("Additional fields to describe the Evaluator. Helpful to separate Evaluator versions from each other with details on how they were created or used.").optional(), "options": z.array(z.object({ "name": z.string().describe("The name of the option."), "valence": z.string().optional() })).describe("The options that can be applied as judgments. Only for Evaluators with `return_type` of 'boolean', 'select' or 'multi_select'.").optional(), "number_limits": z.string().optional(), "number_valence": z.string().optional(), "evaluator_type": z.literal("external").describe("The type of the evaluator.") })]).optional()
}