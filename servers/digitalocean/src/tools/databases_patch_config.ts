import { z } from "zod"

export const toolName = `databases_patch_config`
export const toolDescription = `Update the Database Configuration for an Existing Database`
export const baseUrl = `https://api.digitalocean.com`
export const path = `/v2/databases/{database_cluster_uuid}/config`
export const method = `patch`
export const security = [
  {
    "key": "Authorization",
    "value": "Bearer <mcp-env-var>API_KEY</mcp-env-var>",
    "in": "header",
    "envVarName": "API_KEY",
    "schemeType": "http",
    "schemeScheme": "bearer"
  }
]
export const keys = {
  "query": [],
  "header": [],
  "path": [
    "database_cluster_uuid"
  ],
  "cookie": [],
  "body": [
    "config"
  ]
}
export const flatMap = {}

export const inputParams = {
  "database_cluster_uuid": z.string().uuid().describe("A unique identifier for a database cluster."),
  "config": z.union([z.object({ "backup_hour": z.number().int().gte(0).lte(23).describe("The hour of day (in UTC) when backup for the service starts. New backup only starts if previous backup has already completed.").optional(), "backup_minute": z.number().int().gte(0).lte(59).describe("The minute of the backup hour when backup for the service starts. New backup  only starts if previous backup has already completed.").optional(), "sql_mode": z.string().regex(new RegExp("^[A-Z_]*(,[A-Z_]+)*$")).max(1024).describe("Global SQL mode. If empty, uses MySQL server defaults. Must only include uppercase alphabetic characters, underscores, and commas.").optional(), "connect_timeout": z.number().int().gte(2).lte(3600).describe("The number of seconds that the mysqld server waits for a connect packet before responding with bad handshake.").optional(), "default_time_zone": z.string().min(2).max(100).describe("Default server time zone, in the form of an offset from UTC (from -12:00 to +12:00), a time zone name (EST), or 'SYSTEM' to use the MySQL server default.").optional(), "group_concat_max_len": z.number().int().gte(4).lte(18446744073709552000).describe("The maximum permitted result length, in bytes, for the GROUP_CONCAT() function.").optional(), "information_schema_stats_expiry": z.number().int().gte(900).lte(31536000).describe("The time, in seconds, before cached statistics expire.").optional(), "innodb_ft_min_token_size": z.number().int().gte(0).lte(16).describe("The minimum length of words that an InnoDB FULLTEXT index stores.").optional(), "innodb_ft_server_stopword_table": z.string().regex(new RegExp("^.+/.+$")).max(1024).describe("The InnoDB FULLTEXT index stopword list for all InnoDB tables.").optional(), "innodb_lock_wait_timeout": z.number().int().gte(1).lte(3600).describe("The time, in seconds, that an InnoDB transaction waits for a row lock. before giving up.").optional(), "innodb_log_buffer_size": z.number().int().gte(1048576).lte(4294967295).describe("The size of the buffer, in bytes, that InnoDB uses to write to the log files. on disk.").optional(), "innodb_online_alter_log_max_size": z.number().int().gte(65536).lte(1099511627776).describe("The upper limit, in bytes, of the size of the temporary log files used during online DDL operations for InnoDB tables.").optional(), "innodb_print_all_deadlocks": z.boolean().describe("When enabled, records information about all deadlocks in InnoDB user transactions  in the error log. Disabled by default.").optional(), "innodb_rollback_on_timeout": z.boolean().describe("When enabled, transaction timeouts cause InnoDB to abort and roll back the entire transaction.").optional(), "interactive_timeout": z.number().int().gte(30).lte(604800).describe("The time, in seconds, the server waits for activity on an interactive. connection before closing it.").optional(), "internal_tmp_mem_storage_engine": z.enum(["TempTable","MEMORY"]).describe("The storage engine for in-memory internal temporary tables.").optional(), "net_read_timeout": z.number().int().gte(1).lte(3600).describe("The time, in seconds, to wait for more data from an existing connection. aborting the read.").optional(), "net_write_timeout": z.number().int().gte(1).lte(3600).describe("The number of seconds to wait for a block to be written to a connection before aborting the write.").optional(), "sql_require_primary_key": z.boolean().describe("Require primary key to be defined for new tables or old tables modified with ALTER TABLE and fail if missing. It is recommended to always have primary keys because various functionality may break if any large table is missing them.").optional(), "wait_timeout": z.number().int().gte(1).lte(2147483).describe("The number of seconds the server waits for activity on a noninteractive connection before closing it.").optional(), "max_allowed_packet": z.number().int().gte(102400).lte(1073741824).describe("The size of the largest message, in bytes, that can be received by the server. Default is 67108864 (64M).").optional(), "max_heap_table_size": z.number().int().gte(1048576).lte(1073741824).describe("The maximum size, in bytes, of internal in-memory tables. Also set tmp_table_size. Default is 16777216 (16M)").optional(), "sort_buffer_size": z.number().int().gte(32768).lte(1073741824).describe("The sort buffer size, in bytes, for ORDER BY optimization. Default is 262144. (256K).").optional(), "tmp_table_size": z.number().int().gte(1048576).lte(1073741824).describe("The maximum size, in bytes, of internal in-memory tables. Also set max_heap_table_size. Default is 16777216 (16M).").optional(), "slow_query_log": z.boolean().describe("When enabled, captures slow queries. When disabled, also truncates the mysql.slow_log table. Default is false.").optional(), "long_query_time": z.number().gte(0).lte(3600).describe("The time, in seconds, for a query to take to execute before  being captured by slow_query_logs. Default is 10 seconds.").optional(), "binlog_retention_period": z.number().gte(600).lte(86400).describe("The minimum amount of time, in seconds, to keep binlog entries before deletion.  This may be extended for services that require binlog entries for longer than the default, for example if using the MySQL Debezium Kafka connector.").optional(), "innodb_change_buffer_max_size": z.number().int().gte(0).lte(50).describe("Specifies the maximum size of the InnoDB change buffer as a percentage of the buffer pool.").optional(), "innodb_flush_neighbors": z.union([z.literal(0), z.literal(1), z.literal(2)]).describe("Specifies whether flushing a page from the InnoDB buffer pool also flushes other dirty pages in the same extent.\n  - 0 &mdash; disables this functionality, dirty pages in the same extent are not flushed.\n  - 1 &mdash; flushes contiguous dirty pages in the same extent.\n  - 2 &mdash; flushes dirty pages in the same extent.").optional(), "innodb_read_io_threads": z.number().int().gte(1).lte(64).describe("The number of I/O threads for read operations in InnoDB. Changing this parameter will lead to a restart of the MySQL service.").optional(), "innodb_write_io_threads": z.number().int().gte(1).lte(64).describe("The number of I/O threads for write operations in InnoDB. Changing this parameter will lead to a restart of the MySQL service.").optional(), "innodb_thread_concurrency": z.number().int().gte(0).lte(1000).describe("Defines the maximum number of threads permitted inside of InnoDB. A value of 0 (the default) is interpreted as infinite concurrency (no limit). This variable is intended for performance  tuning on high concurrency systems.").optional(), "net_buffer_length": z.number().int().gte(1024).lte(1048576).describe("Start sizes of connection buffer and result buffer, must be multiple of 1024. Changing this parameter will lead to a restart of the MySQL service.").optional(), "log_output": z.enum(["INSIGHTS","TABLE","INSIGHTS,TABLE","NONE"]).describe("Defines the destination for logs. Can be `INSIGHTS`, `TABLE`, or both (`INSIGHTS,TABLE`), or `NONE` to disable logs. To specify both destinations, use `INSIGHTS,TABLE` (order matters). Default is NONE.") }), z.object({ "autovacuum_freeze_max_age": z.number().int().gte(200000000).lte(1500000000).describe("Specifies the maximum age (in transactions) that a table's pg_class.relfrozenxid field can attain before a VACUUM operation is forced to prevent transaction ID wraparound within the table. Note that the system will launch autovacuum processes to prevent wraparound even when autovacuum is otherwise disabled. This parameter will cause the server to be restarted.").optional(), "autovacuum_max_workers": z.number().int().gte(1).lte(20).describe("Specifies the maximum number of autovacuum processes (other than the autovacuum launcher) that may be running at any one time. The default is three. This parameter can only be set at server start.").optional(), "autovacuum_naptime": z.number().int().gte(0).lte(86400).describe("Specifies the minimum delay, in seconds, between autovacuum runs on any given database. The default is one minute.").optional(), "autovacuum_vacuum_threshold": z.number().int().gte(0).lte(2147483647).describe("Specifies the minimum number of updated or deleted tuples needed to trigger a VACUUM in any one table. The default is 50 tuples.").optional(), "autovacuum_analyze_threshold": z.number().int().gte(0).lte(2147483647).describe("Specifies the minimum number of inserted, updated, or deleted tuples needed to trigger an ANALYZE in any one table. The default is 50 tuples.").optional(), "autovacuum_vacuum_scale_factor": z.number().gte(0).lte(1).describe("Specifies a fraction, in a decimal value, of the table size to add to autovacuum_vacuum_threshold when deciding whether to trigger a VACUUM. The default is 0.2 (20% of table size).").optional(), "autovacuum_analyze_scale_factor": z.number().gte(0).lte(1).describe("Specifies a fraction, in a decimal value, of the table size to add to autovacuum_analyze_threshold when deciding whether to trigger an ANALYZE. The default is 0.2 (20% of table size).").optional(), "autovacuum_vacuum_cost_delay": z.number().int().gte(-1).lte(100).describe("Specifies the cost delay value, in milliseconds, that will be used in automatic VACUUM operations. If -1, uses the regular vacuum_cost_delay value, which is 20 milliseconds.").optional(), "autovacuum_vacuum_cost_limit": z.number().int().gte(-1).lte(10000).describe("Specifies the cost limit value that will be used in automatic VACUUM operations. If -1 is specified (which is the default), the regular vacuum_cost_limit value will be used.").optional(), "backup_hour": z.number().int().gte(0).lte(23).describe("The hour of day (in UTC) when backup for the service starts. New backup only starts if previous backup has already completed.").optional(), "backup_minute": z.number().int().gte(0).lte(59).describe("The minute of the backup hour when backup for the service starts. New backup is only started if previous backup has already completed.").optional(), "bgwriter_delay": z.number().int().gte(10).lte(10000).describe("Specifies the delay, in milliseconds, between activity rounds for the background writer. Default is 200 ms.").optional(), "bgwriter_flush_after": z.number().int().gte(0).lte(2048).describe("The amount of kilobytes that need to be written by the background writer before attempting to force the OS to issue these writes to underlying storage. Specified in kilobytes, default is 512.  Setting of 0 disables forced writeback.").optional(), "bgwriter_lru_maxpages": z.number().int().gte(0).lte(1073741823).describe("The maximum number of buffers that the background writer can write. Setting this to zero disables background writing. Default is 100.").optional(), "bgwriter_lru_multiplier": z.number().gte(0).lte(10).describe("The average recent need for new buffers is multiplied by bgwriter_lru_multiplier to arrive at an estimate of the number that will be needed during the next round, (up to bgwriter_lru_maxpages). 1.0 represents a “just in time” policy of writing exactly the number of buffers predicted to be needed. Larger values provide some cushion against spikes in demand, while smaller values intentionally leave writes to be done by server processes. The default is 2.0.").optional(), "deadlock_timeout": z.number().int().gte(500).lte(1800000).describe("The amount of time, in milliseconds, to wait on a lock before checking to see if there is a deadlock condition.").optional(), "default_toast_compression": z.enum(["lz4","pglz"]).describe("Specifies the default TOAST compression method for values of compressible columns (the default is lz4).").optional(), "idle_in_transaction_session_timeout": z.number().int().gte(0).lte(604800000).describe("Time out sessions with open transactions after this number of milliseconds").optional(), "jit": z.boolean().describe("Activates, in a boolean, the system-wide use of Just-in-Time Compilation (JIT).").optional(), "log_autovacuum_min_duration": z.number().int().gte(-1).lte(2147483647).describe("Causes each action executed by autovacuum to be logged if it ran for at least the specified number of milliseconds. Setting this to zero logs all autovacuum actions. Minus-one (the default) disables logging autovacuum actions.").optional(), "log_error_verbosity": z.enum(["TERSE","DEFAULT","VERBOSE"]).describe("Controls the amount of detail written in the server log for each message that is logged.").optional(), "log_line_prefix": z.enum(["pid=%p,user=%u,db=%d,app=%a,client=%h","%m [%p] %q[user=%u,db=%d,app=%a]","%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h"]).describe("Selects one of the available log-formats. These can support popular log analyzers like pgbadger, pganalyze, etc.").optional(), "log_min_duration_statement": z.number().int().gte(-1).lte(86400000).describe("Log statements that take more than this number of milliseconds to run. If -1, disables.").optional(), "max_files_per_process": z.number().int().gte(1000).lte(4096).describe("PostgreSQL maximum number of files that can be open per process.").optional(), "max_prepared_transactions": z.number().int().gte(0).lte(10000).describe("PostgreSQL maximum prepared transactions. Once increased, this parameter cannot be lowered from its set value.").optional(), "max_pred_locks_per_transaction": z.number().int().gte(64).lte(640).describe("PostgreSQL maximum predicate locks per transaction.").optional(), "max_locks_per_transaction": z.number().int().gte(64).lte(6400).describe("PostgreSQL maximum locks per transaction. Once increased, this parameter cannot be lowered from its set value.").optional(), "max_stack_depth": z.number().int().gte(2097152).lte(6291456).describe("Maximum depth of the stack in bytes.").optional(), "max_standby_archive_delay": z.number().int().gte(1).lte(43200000).describe("Max standby archive delay in milliseconds.").optional(), "max_standby_streaming_delay": z.number().int().gte(1).lte(43200000).describe("Max standby streaming delay in milliseconds.").optional(), "max_replication_slots": z.number().int().gte(8).lte(64).describe("PostgreSQL maximum replication slots.").optional(), "max_logical_replication_workers": z.number().int().gte(4).lte(64).describe("PostgreSQL maximum logical replication workers (taken from the pool of max_parallel_workers).").optional(), "max_parallel_workers": z.number().int().gte(0).lte(96).describe("Sets the maximum number of workers that the system can support for parallel queries.").optional(), "max_parallel_workers_per_gather": z.number().int().gte(0).lte(96).describe("Sets the maximum number of workers that can be started by a single Gather or Gather Merge node.").optional(), "max_worker_processes": z.number().int().gte(8).lte(96).describe("Sets the maximum number of background processes that the system can support. Once increased, this parameter cannot be lowered from its set value.").optional(), "pg_partman_bgw.role": z.string().regex(new RegExp("^[_A-Za-z0-9][-._A-Za-z0-9]{0,63}$")).max(64).describe("Controls which role to use for pg_partman's scheduled background tasks. Must consist of alpha-numeric characters, dots, underscores, or dashes. May not start with dash or dot. Maximum of 64 characters.").optional(), "pg_partman_bgw.interval": z.number().int().gte(3600).lte(604800).describe("Sets the time interval to run pg_partman's scheduled tasks.").optional(), "pg_stat_statements.track": z.enum(["all","top","none"]).describe("Controls which statements are counted. Specify 'top' to track top-level statements (those issued directly by clients), 'all' to also track nested statements (such as statements invoked within functions), or 'none' to disable statement statistics collection. The default value is top.").optional(), "temp_file_limit": z.number().int().gte(-1).lte(2147483647).describe("PostgreSQL temporary file limit in KiB. If -1, sets to unlimited.").optional(), "timezone": z.string().max(64).describe("PostgreSQL service timezone").optional(), "track_activity_query_size": z.number().int().gte(1024).lte(10240).describe("Specifies the number of bytes reserved to track the currently executing command for each active session.").optional(), "track_commit_timestamp": z.enum(["off","on"]).describe("Record commit time of transactions.").optional(), "track_functions": z.enum(["all","pl","none"]).describe("Enables tracking of function call counts and time used.").optional(), "track_io_timing": z.enum(["off","on"]).describe("Enables timing of database I/O calls. This parameter is off by default, because it will repeatedly query the operating system for the current time, which may cause significant overhead on some platforms.").optional(), "max_wal_senders": z.number().int().gte(20).lte(64).describe("PostgreSQL maximum WAL senders. Once increased, this parameter cannot be lowered from its set value.").optional(), "wal_sender_timeout": z.number().int().gte(0).lte(10800000).describe("Terminate replication connections that are inactive for longer than this amount of time, in milliseconds. Setting this value to zero disables the timeout. Must be either 0 or between 5000 and 10800000.").optional(), "wal_writer_delay": z.number().int().gte(10).lte(200).describe("WAL flush interval in milliseconds. Note that setting this value to lower than the default 200ms may negatively impact performance").optional(), "shared_buffers_percentage": z.number().gte(20).lte(60).describe("Percentage of total RAM that the database server uses for shared memory buffers.  Valid range is 20-60 (float), which corresponds to 20% - 60%.  This setting adjusts the shared_buffers configuration value.").optional(), "pgbouncer": z.object({ "server_reset_query_always": z.boolean().describe("Run server_reset_query (DISCARD ALL) in all pooling modes.").optional(), "ignore_startup_parameters": z.array(z.enum(["extra_float_digits","search_path"]).describe("Enum of parameters to ignore when given in startup packet.")).max(32).describe("List of parameters to ignore when given in startup packet.").optional(), "min_pool_size": z.number().int().gte(0).lte(10000).describe("If current server connections are below this number, adds more. Improves behavior when usual load comes suddenly back after period of total inactivity. The value is effectively capped at the pool size.").optional(), "server_lifetime": z.number().int().gte(60).lte(86400).describe("The pooler closes any unused server connection that has been connected longer than this amount of seconds.").optional(), "server_idle_timeout": z.number().int().gte(0).lte(86400).describe("Drops server connections if they have been idle more than this many seconds.  If 0, timeout is disabled. ").optional(), "autodb_pool_size": z.number().int().gte(0).lte(10000).describe("If non-zero, automatically creates a pool of that size per user when a pool doesn't exist.").optional(), "autodb_pool_mode": z.enum(["session","transaction","statement"]).describe("PGBouncer pool mode").optional(), "autodb_max_db_connections": z.number().int().gte(0).lte(2147483647).describe("Only allows a maximum this many server connections per database (regardless of user). If 0, allows unlimited connections.").optional(), "autodb_idle_timeout": z.number().int().gte(0).lte(86400).describe("If the automatically-created database pools have been unused this many seconds, they are freed. If 0, timeout is disabled.").optional() }).describe("PGBouncer connection pooling settings").optional(), "work_mem": z.number().int().gte(1).lte(1024).describe("The maximum amount of memory, in MB, used by a query operation (such as a sort or hash table) before writing to temporary disk files. Default is 1MB + 0.075% of total RAM (up to 32MB).").optional(), "timescaledb": z.object({ "max_background_workers": z.number().int().gte(1).lte(4096).describe("The number of background workers for timescaledb operations.  Set to the sum of your number of databases and the total number of concurrent background workers you want running at any given point in time.").optional() }).describe("TimescaleDB extension configuration values").optional(), "synchronous_replication": z.enum(["off","quorum"]).describe("Synchronous replication type. Note that the service plan also needs to support synchronous replication.").optional(), "stat_monitor_enable": z.boolean().describe("Enable the pg_stat_monitor extension. <b>Enabling this extension will cause the cluster to be restarted.</b> When this extension is enabled, pg_stat_statements results for utility commands are unreliable.").optional(), "max_failover_replication_time_lag": z.number().int().gte(10).lte(9223372036854776000).describe("Number of seconds of master unavailability before triggering database failover to standby. The default value is 60.").optional() }), z.object({ "redis_maxmemory_policy": z.enum(["noeviction","allkeys-lru","allkeys-random","volatile-lru","volatile-random","volatile-ttl"]).describe("A string specifying the desired eviction policy for the Redis cluster.\n\n- `noeviction`: Don't evict any data, returns error when memory limit is reached.\n- `allkeys-lru:` Evict any key, least recently used (LRU) first.\n- `allkeys-random`: Evict keys in a random order.\n- `volatile-lru`: Evict keys with expiration only, least recently used (LRU) first.\n- `volatile-random`: Evict keys with expiration only in a random order.\n- `volatile-ttl`: Evict keys with expiration only, shortest time-to-live (TTL) first.").optional(), "redis_pubsub_client_output_buffer_limit": z.number().int().gte(32).lte(512).describe("Set output buffer limit for pub / sub clients in MB. The value is the hard limit, the soft limit is 1/4 of the hard limit. When setting the limit, be mindful of the available memory in the selected service plan.").optional(), "redis_number_of_databases": z.number().int().gte(1).lte(128).describe("Set number of redis databases. Changing this will cause a restart of redis service.").optional(), "redis_io_threads": z.number().int().gte(1).lte(32).describe("Redis IO thread count").optional(), "redis_lfu_log_factor": z.number().int().gte(0).lte(100).describe("Counter logarithm factor for volatile-lfu and allkeys-lfu maxmemory-policies"), "redis_lfu_decay_time": z.number().int().gte(1).lte(120).describe("LFU maxmemory-policy counter decay time in minutes"), "redis_ssl": z.boolean().describe("Require SSL to access Redis.\n- When enabled, Redis accepts only SSL connections on port `25061`.\n- When disabled, port `25060` is opened for non-SSL connections, while port `25061` remains available for SSL connections.\n"), "redis_timeout": z.number().int().gte(0).lte(31536000).describe("Redis idle connection timeout in seconds"), "redis_notify_keyspace_events": z.string().regex(new RegExp("^[KEg\\$lshzxeA]*$")).max(32).describe("Set notify-keyspace-events option. Requires at least `K` or `E` and accepts any combination of the following options. Setting the parameter to `\"\"` disables notifications.\n- `K` &mdash; Keyspace events\n- `E` &mdash; Keyevent events\n- `g` &mdash; Generic commands (e.g. `DEL`, `EXPIRE`, `RENAME`, ...)\n- `{
  "database_cluster_uuid": z.string().uuid().describe("A unique identifier for a database cluster."),
  "config":  &mdash; String commands\n- `l` &mdash; List commands\n- `s` &mdash; Set commands\n- `h` &mdash; Hash commands\n- `z` &mdash; Sorted set commands\n- `t` &mdash; Stream commands\n- `d` &mdash; Module key type events\n- `x` &mdash; Expired events\n- `e` &mdash; Evicted events\n- `m` &mdash; Key miss events\n- `n` &mdash; New key events\n- `A` &mdash; Alias for `\"g$lshztxed\"`"), "redis_persistence": z.enum(["off","rdb"]).describe("Creates an RDB dump of the database every 10 minutes that can be used  to recover data after a node crash. The database does not create the  dump if no keys have changed since the last dump. When set to `off`,  the database cannot fork services, and data can be lost if a service  is restarted or powered off. DigitalOcean Managed Caching databases  do not support the Append Only File (AOF) persistence method.").optional(), "redis_acl_channels_default": z.enum(["allchannels","resetchannels"]).describe("Determines default pub/sub channels' ACL for new users if ACL is not supplied. When this option is not defined, all_channels is assumed to keep backward compatibility. This option doesn't affect Redis configuration acl-pubsub-default.").optional() }), z.object({ "default_read_concern": z.enum(["local","available","majority"]).describe("Specifies the default consistency behavior of reads from the database. Data that is returned from the query with may or may not have been acknowledged by all nodes in the replicaset depending on this value.  Learn more [here](https://www.mongodb.com/docs/manual/reference/read-concern/)."), "default_write_concern": z.string().describe("Describes the level of acknowledgment requested from MongoDB for write operations clusters. This field can set to either `majority` or a number `0...n` which will describe the number of nodes that must acknowledge the write operation before it is fully accepted. Setting to `0` will request no acknowledgement of the write operation.  Learn more [here](https://www.mongodb.com/docs/manual/reference/write-concern/)."), "transaction_lifetime_limit_seconds": z.number().int().gte(1).describe("Specifies the lifetime of multi-document transactions. Transactions that exceed this limit are considered expired and will be  aborted by a periodic cleanup process. The cleanup process runs every `transactionLifetimeLimitSeconds/2 seconds` or at least  once every 60 seconds. *Changing this parameter will lead to a restart of the MongoDB service.* Learn more [here](https://www.mongodb.com/docs/manual/reference/parameters/#mongodb-parameter-param.transactionLifetimeLimitSeconds)."), "slow_op_threshold_ms": z.number().int().gte(0).describe("Operations that run for longer than this threshold are considered slow which are then recorded to the diagnostic logs.  Higher log levels (verbosity) will record all operations regardless of this threshold on the primary node.  *Changing this parameter will lead to a restart of the MongoDB service.* Learn more [here](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-operationProfiling.slowOpThresholdMs)."), "verbosity": z.number().int().gte(0).lte(5).describe("The log message verbosity level. The verbosity level determines the amount of Informational and Debug messages MongoDB outputs. 0 includes informational messages while 1...5 increases the level to include debug messages. *Changing this parameter will lead to a restart of the MongoDB service.* Learn more [here](https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-systemLog.verbosity).") }), z.object({ "compression_type": z.enum(["gzip","snappy","lz4","zstd","uncompressed","producer"]).describe("Specify the final compression type for a given topic. This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd'). It additionally accepts 'uncompressed' which is equivalent to no compression; and 'producer' which means retain the original compression codec set by the producer.").optional(), "group_initial_rebalance_delay_ms": z.number().int().gte(0).lte(300000).describe("The amount of time, in milliseconds, the group coordinator will wait for more consumers to join a new group before performing the first rebalance. A longer delay means potentially fewer rebalances, but increases the time until processing begins. The default value for this is 3 seconds. During development and testing it might be desirable to set this to 0 in order to not delay test execution time.").optional(), "group_min_session_timeout_ms": z.number().int().gte(0).lte(60000).describe("The minimum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.").optional(), "group_max_session_timeout_ms": z.number().int().gte(0).lte(1800000).describe("The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.").optional(), "connections_max_idle_ms": z.number().int().gte(1000).lte(3600000).describe("Idle connections timeout: the server socket processor threads close the connections that idle for longer than this.").optional(), "max_incremental_fetch_session_cache_slots": z.number().int().gte(1000).lte(10000).describe("The maximum number of incremental fetch sessions that the broker will maintain.").optional(), "message_max_bytes": z.number().int().gte(0).lte(100001200).describe("The maximum size of message that the server can receive.").optional(), "offsets_retention_minutes": z.number().int().gte(1).lte(2147483647).describe("Log retention window in minutes for offsets topic").optional(), "log_cleaner_delete_retention_ms": z.number().int().gte(0).lte(315569260000).describe("How long are delete records retained?").optional(), "log_cleaner_min_cleanable_ratio": z.number().gte(0.2).lte(0.9).describe("Controls log compactor frequency. Larger value means more frequent compactions but also more space wasted for logs. Consider setting log_cleaner_max_compaction_lag_ms to enforce compactions sooner, instead of setting a very high value for this option.").optional(), "log_cleaner_max_compaction_lag_ms": z.number().int().gte(30000).lte(9223372036854776000).describe("The maximum amount of time message will remain uncompacted. Only applicable for logs that are being compacted").optional(), "log_cleaner_min_compaction_lag_ms": z.number().int().gte(0).lte(9223372036854776000).describe("The minimum time a message will remain uncompacted in the log. Only applicable for logs that are being compacted.").optional(), "log_cleanup_policy": z.enum(["delete","compact","compact,delete"]).describe("The default cleanup policy for segments beyond the retention window").optional(), "log_flush_interval_messages": z.number().int().gte(1).lte(9223372036854776000).describe("The number of messages accumulated on a log partition before messages are flushed to disk").optional(), "log_flush_interval_ms": z.number().int().gte(0).lte(9223372036854776000).describe("The maximum time in ms that a message in any topic is kept in memory before flushed to disk. If not set, the value in log.flush.scheduler.interval.ms is used").optional(), "log_index_interval_bytes": z.number().int().gte(0).lte(104857600).describe("The interval with which Kafka adds an entry to the offset index").optional(), "log_index_size_max_bytes": z.number().int().gte(1048576).lte(104857600).describe("The maximum size in bytes of the offset index").optional(), "log_message_downconversion_enable": z.boolean().describe("This configuration controls whether down-conversion of message formats is enabled to satisfy consume requests.").optional(), "log_message_timestamp_type": z.enum(["CreateTime","LogAppendTime"]).describe("Define whether the timestamp in the message is message create time or log append time.").optional(), "log_message_timestamp_difference_max_ms": z.number().int().gte(0).lte(9223372036854776000).describe("The maximum difference allowed between the timestamp when a broker receives a message and the timestamp specified in the message").optional(), "log_preallocate": z.boolean().describe("Controls whether to preallocate a file when creating a new segment").optional(), "log_retention_bytes": z.number().int().gte(-1).lte(9223372036854776000).describe("The maximum size of the log before deleting messages").optional(), "log_retention_hours": z.number().int().gte(-1).lte(2147483647).describe("The number of hours to keep a log file before deleting it").optional(), "log_retention_ms": z.number().int().gte(-1).lte(9223372036854776000).describe("The number of milliseconds to keep a log file before deleting it (in milliseconds), If not set, the value in log.retention.minutes is used. If set to -1, no time limit is applied.").optional(), "log_roll_jitter_ms": z.number().int().gte(0).lte(9223372036854776000).describe("The maximum jitter to subtract from logRollTimeMillis (in milliseconds). If not set, the value in log.roll.jitter.hours is used").optional(), "log_roll_ms": z.number().int().gte(1).lte(9223372036854776000).describe("The maximum time before a new log segment is rolled out (in milliseconds).").optional(), "log_segment_bytes": z.number().int().gte(10485760).lte(1073741824).describe("The maximum size of a single log file").optional(), "log_segment_delete_delay_ms": z.number().int().gte(0).lte(3600000).describe("The amount of time to wait before deleting a file from the filesystem").optional(), "auto_create_topics_enable": z.boolean().describe("Enable auto creation of topics").optional(), "min_insync_replicas": z.number().int().gte(1).lte(7).describe("When a producer sets acks to 'all' (or '-1'), min_insync_replicas specifies the minimum number of replicas that must acknowledge a write for the write to be considered successful.").optional(), "num_partitions": z.number().int().gte(1).lte(1000).describe("Number of partitions for autocreated topics").optional(), "default_replication_factor": z.number().int().gte(1).lte(10).describe("Replication factor for autocreated topics").optional(), "replica_fetch_max_bytes": z.number().int().gte(1048576).lte(104857600).describe("The number of bytes of messages to attempt to fetch for each partition (defaults to 1048576). This is not an absolute maximum, if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made.").optional(), "replica_fetch_response_max_bytes": z.number().int().gte(10485760).lte(1048576000).describe("Maximum bytes expected for the entire fetch response (defaults to 10485760). Records are fetched in batches, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made. As such, this is not an absolute maximum.").optional(), "max_connections_per_ip": z.number().int().gte(256).lte(2147483647).describe("The maximum number of connections allowed from each ip address (defaults to 2147483647).").optional(), "producer_purgatory_purge_interval_requests": z.number().int().gte(10).lte(10000).describe("The purge interval (in number of requests) of the producer request purgatory (defaults to 1000).").optional(), "socket_request_max_bytes": z.number().int().gte(10485760).lte(209715200).describe("The maximum number of bytes in a socket request (defaults to 104857600).").optional(), "transaction_state_log_segment_bytes": z.number().int().gte(1048576).lte(2147483647).describe("The transaction topic segment bytes should be kept relatively small in order to facilitate faster log compaction and cache loads (defaults to 104857600 (100 mebibytes)).").optional(), "transaction_remove_expired_transaction_cleanup_interval_ms": z.number().int().gte(600000).lte(3600000).describe("The interval at which to remove transactions that have expired due to transactional.id.expiration.ms passing (defaults to 3600000 (1 hour)).").optional() }), z.object({ "http_max_content_length_bytes": z.number().int().gte(1).lte(2147483647).describe("Maximum content length for HTTP requests to the OpenSearch HTTP API, in bytes."), "http_max_header_size_bytes": z.number().int().gte(1024).lte(262144).describe("Maximum size of allowed headers, in bytes."), "http_max_initial_line_length_bytes": z.number().int().gte(1024).lte(65536).describe("Maximum length of an HTTP URL, in bytes."), "indices_query_bool_max_clause_count": z.number().int().gte(64).lte(4096).describe("Maximum number of clauses Lucene BooleanQuery can have.  Only increase it if necessary, as it may cause performance issues."), "indices_fielddata_cache_size_percentage": z.number().int().gte(3).lte(100).describe("Maximum amount of heap memory used for field data cache, expressed as a percentage. Decreasing the value too much will increase overhead of loading field data. Increasing the value too much will decrease amount of heap available for other operations.").optional(), "indices_memory_index_buffer_size_percentage": z.number().int().gte(3).lte(40).describe("Total amount of heap used for indexing buffer before writing segments to disk, expressed as a percentage. Too low value will slow down indexing; too high value will increase indexing performance but causes performance issues for query performance."), "indices_memory_min_index_buffer_size_mb": z.number().int().gte(3).lte(2048).describe("Minimum amount of heap used for indexing buffer before writing segments to disk, in mb. Works in conjunction with indices_memory_index_buffer_size_percentage, each being enforced."), "indices_memory_max_index_buffer_size_mb": z.number().int().gte(3).lte(2048).describe("Maximum amount of heap used for indexing buffer before writing segments to disk, in mb. Works in conjunction with indices_memory_index_buffer_size_percentage, each being enforced. The default is unbounded.").optional(), "indices_queries_cache_size_percentage": z.number().int().gte(3).lte(40).describe("Maximum amount of heap used for query cache.  Too low value will decrease query performance and increase performance for other operations; too high value will cause issues with other functionality."), "indices_recovery_max_mb_per_sec": z.number().int().gte(40).lte(400).describe("Limits total inbound and outbound recovery traffic for each node, expressed in mb per second. Applies to both peer recoveries as well as snapshot recoveries (i.e., restores from a snapshot)."), "indices_recovery_max_concurrent_file_chunks": z.number().int().gte(2).lte(5).describe("Maximum number of file chunks sent in parallel for each recovery."), "thread_pool_search_size": z.number().int().gte(1).lte(128).describe("Number of workers in the search operation thread pool.  Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.").optional(), "thread_pool_search_throttled_size": z.number().int().gte(1).lte(128).describe("Number of workers in the search throttled operation thread pool. This pool is used for searching frozen indices. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.").optional(), "thread_pool_get_size": z.number().int().gte(1).lte(128).describe("Number of workers in the get operation thread pool.  Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.").optional(), "thread_pool_analyze_size": z.number().int().gte(1).lte(128).describe("Number of workers in the analyze operation thread pool.  Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.").optional(), "thread_pool_write_size": z.number().int().gte(1).lte(128).describe("Number of workers in the write operation thread pool.  Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.").optional(), "thread_pool_force_merge_size": z.number().int().gte(1).lte(128).describe("Number of workers in the force merge operation thread pool. This pool is used for forcing a merge between shards of one or more indices. Do note this may have maximum value depending on CPU count - value is automatically lowered if set to higher than maximum value.").optional(), "thread_pool_search_queue_size": z.number().int().gte(10).lte(2000).describe("Size of queue for operations in the search thread pool.").optional(), "thread_pool_search_throttled_queue_size": z.number().int().gte(10).lte(2000).describe("Size of queue for operations in the search throttled thread pool.").optional(), "thread_pool_get_queue_size": z.number().int().gte(10).lte(2000).describe("Size of queue for operations in the get thread pool.").optional(), "thread_pool_analyze_queue_size": z.number().int().gte(10).lte(2000).describe("Size of queue for operations in the analyze thread pool.").optional(), "thread_pool_write_queue_size": z.number().int().gte(10).lte(2000).describe("Size of queue for operations in the write thread pool.").optional(), "ism_enabled": z.boolean().describe("Specifies whether ISM is enabled or not."), "ism_history_enabled": z.boolean().describe("Specifies whether audit history is enabled or not. The logs from ISM are automatically indexed to a logs document."), "ism_history_max_age_hours": z.number().int().gte(1).lte(2147483647).describe("Maximum age before rolling over the audit history index, in hours."), "ism_history_max_docs": z.number().int().gte(1).lte(9223372036854776000).describe("Maximum number of documents before rolling over the audit history index."), "ism_history_rollover_check_period_hours": z.number().int().gte(1).lte(2147483647).describe("The time between rollover checks for the audit history index, in hours."), "ism_history_rollover_retention_period_days": z.number().int().gte(1).lte(2147483647).describe("Length of time long audit history indices are kept, in days."), "search_max_buckets": z.number().int().gte(1).lte(1000000).describe("Maximum number of aggregation buckets allowed in a single response."), "action_auto_create_index_enabled": z.boolean().describe("Specifices whether to allow automatic creation of indices."), "enable_security_audit": z.boolean().describe("Specifies whether to allow security audit logging."), "action_destructive_requires_name": z.boolean().describe("Specifies whether to require explicit index names when deleting indices.").optional(), "cluster_max_shards_per_node": z.number().int().gte(100).lte(10000).describe("Maximum number of shards allowed per data node.").optional(), "override_main_response_version": z.boolean().describe("Compatibility mode sets OpenSearch to report its version as 7.10 so clients continue to work."), "script_max_compilations_rate": z.string().describe("Limits the number of inline script compilations within a period of time. Default is use-context"), "cluster_routing_allocation_node_concurrent_recoveries": z.number().int().gte(2).lte(16).describe("Maximum concurrent incoming/outgoing shard recoveries (normally replicas) are allowed to happen per node ."), "reindex_remote_whitelist": z.array(z.string()).describe("Allowlist of remote IP addresses for reindexing. Changing this value will cause all OpenSearch instances to restart.").optional(), "plugins_alerting_filter_by_backend_roles_enabled": z.boolean().describe("Enable or disable filtering of alerting by backend roles.") })]).optional()
}